# üñ• **Cybersecurity Desktop Exercise ‚Äî Metrics Dashboard Template**

This template helps facilitators track key performance indicators (KPIs) during the exercise. By checking off each metric in real-time, facilitators can assess how effectively the team responded and provide data-driven insights during the debriefing.

---

### **Metrics Dashboard**

| **Metric**                | **Target**            | **Status** |
| ------------------------- | --------------------- | ---------- |
| **Threat Detection Time** | ‚â§30 min               | ‚òê          |
| **Fix Success Rate**      | 100%                  | ‚òê          |
| **AI Accuracy**           | ‚â•90%                  | ‚òê          |
| **Red-Herring Handling**  | ‚â•80%                  | ‚òê          |
| **Post-Incident Report**  | Complete & Actionable | ‚òê          |

---

### **How to Use:**

* **Track Each Scenario:** As the team progresses through each scenario, facilitators should assess and record the relevant metrics in real time.
* **Checkboxes:** Facilitators will check off each box to indicate whether or not the team met the target for each metric.
* **Debriefing:** At the end of the exercise, facilitators can review the metrics to assess overall team performance and identify areas for improvement.

---

### **Metric Definitions:**

1. **Threat Detection Time**:

   * **Target**: ‚â§30 min
   * **Description**: The time it takes for the team to identify a security threat from the moment it appears. Shorter detection times indicate more effective monitoring and response systems.
   * **Example**: If a phishing attack or ransomware attempt is detected in less than 30 minutes, this metric is met.

2. **Fix Success Rate**:

   * **Target**: 100%
   * **Description**: The percentage of successful fixes in response to identified threats. A higher success rate indicates a well-trained team that is capable of applying effective remediation steps.
   * **Example**: If the team successfully isolates a compromised system and mitigates the threat without further damage, this metric is met.

3. **AI Accuracy**:

   * **Target**: ‚â•90%
   * **Description**: The accuracy of AI tools in assisting with threat detection, classification, and suggested remediations. This metric measures the AI's ability to support the team in identifying and resolving incidents.
   * **Example**: If the AI flags true threats accurately without many false positives or misses, this metric is met.

4. **Red-Herring Handling**:

   * **Target**: ‚â•80%
   * **Description**: The ability of the team to correctly identify and disregard false positives or distractions (red herrings) that do not contribute to the actual incident.
   * **Example**: If the team identifies that a CPU spike is due to a benign process and not a ransomware attack, this metric is met.

5. **Post-Incident Report**:

   * **Target**: Complete & Actionable
   * **Description**: The quality and completeness of the post-incident analysis report. This includes documenting the incident, root cause, actions taken, lessons learned, and recommended preventive measures.
   * **Example**: After the exercise, the team generates a thorough report that includes a **root cause analysis** and specific **next steps** to prevent future occurrences.

---

### **Debriefing & Evaluation:**

At the end of the exercise, facilitators can use the completed dashboard to discuss:

* **Performance Insights:** Which areas did the team excel in, and where did they fall short?
* **AI Effectiveness:** How well did AI assist in threat detection and remediation? Were there any gaps in its support?
* **Red-Herring Handling:** Was the team able to stay focused under pressure? How well did they differentiate between real threats and distractions?
* **Incident Response:** Was the remediation process effective? Did the team take the correct actions in a timely manner?

---

**Tip:** Use the data from this dashboard to **drive improvement** in future training exercises and **optimize security tools** (like AI monitoring) based on real-world performance.
